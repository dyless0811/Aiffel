{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d1a55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "Metal device set to: Apple M1\n",
      "1단계 연산 준비: (64, 4, 2)\n",
      "1단계 연산 결과: (64, 4)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 1)\n",
      "\n",
      "2단계 연산 준비: (64, 4)\n",
      "2단계 연산 결과: (64,)\n",
      "2단계 Linear Layer의 Weight 형태: (4, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 00:08:21.394230: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-08-04 00:08:21.394315: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))     # Tensorflow는 Batch를 기반으로 동작하기에,\n",
    "                                         # 우리는 사각형 2개 세트를 batch_size개만큼\n",
    "                                         # 만든 후 처리를 하게 됩니다.\n",
    "print(\"1단계 연산 준비:\", boxes.shape)\n",
    "\n",
    "first_linear = tf.keras.layers.Dense(units=1, use_bias=False) \n",
    "# units은 출력 차원 수를 의미합니다.\n",
    "# Weight 행렬 속 실수를 인간의 뇌 속 하나의 뉴런 '유닛' 취급을 하는 거죠!\n",
    "\n",
    "first_out = first_linear(boxes)\n",
    "first_out = tf.squeeze(first_out, axis=-1) # (4, 1)을 (4,)로 변환해줍니다.\n",
    "                                           # (불필요한 차원 축소)\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)\n",
    "\n",
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28756ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1단계 연산 준비: (64, 4, 2)\n",
      "1단계 연산 결과: (64, 4, 1)\n",
      "1단계 Linear Layer의 Weight 형태: (2, 1)\n",
      "\n",
      "2단계 연산 준비: (64, 4, 1)\n",
      "2단계 연산 결과: (64, 4)\n",
      "2단계 Linear Layer의 Weight 형태: (1, 1)\n",
      "\n",
      "3단계 연산 준비: (64, 4)\n",
      "3단계 연산 결과: (64,)\n",
      "3단계 Linear Layer의 Weight 형태: (4, 1)\n",
      "총 Parameters: 7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "boxes = tf.zeros((batch_size, 4, 2))\n",
    "\n",
    "print(\"1단계 연산 준비:\", boxes.shape)\n",
    "\n",
    "########\n",
    "# Step 1: (4,2)차원인 boxes를 (4,3)으로 확장시키는 Linear Layer\n",
    "first_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "first_out = first_linear(boxes)\n",
    "\n",
    "########\n",
    "\n",
    "print(\"1단계 연산 결과:\", first_out.shape)\n",
    "print(\"1단계 Linear Layer의 Weight 형태:\", first_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n2단계 연산 준비:\", first_out.shape)\n",
    "\n",
    "# Dense = Linear\n",
    "second_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "second_out = second_linear(first_out)\n",
    "second_out = tf.squeeze(second_out, axis=-1)\n",
    "\n",
    "print(\"2단계 연산 결과:\", second_out.shape)\n",
    "print(\"2단계 Linear Layer의 Weight 형태:\", second_linear.weights[0].shape)\n",
    "\n",
    "print(\"\\n3단계 연산 준비:\", second_out.shape)\n",
    "\n",
    "########\n",
    "# Step 2: 4차원인 second_out을 하나의 실수으로 집약시키는 Linear Layer\n",
    "third_linear = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "third_out = third_linear(second_out)\n",
    "third_out = tf.squeeze(third_out, axis=-1)\n",
    "\n",
    "########\n",
    "\n",
    "print(\"3단계 연산 결과:\", third_out.shape)\n",
    "print(\"3단계 Linear Layer의 Weight 형태:\", third_linear.weights[0].shape)\n",
    "\n",
    "########\n",
    "# Step 3: 모든 params를 더하여 total_parmams\n",
    "total_params = \\\n",
    "first_linear.count_params() + \\\n",
    "second_linear.count_params() + \\\n",
    "third_linear.count_params()\n",
    "\n",
    "########\n",
    "\n",
    "print(\"총 Parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f670a837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 이미지 데이터: (64, 1920, 1080, 3)\n",
      "\n",
      "Convolution 결과: (64, 384, 216, 16)\n",
      "Convolution Layer의 Parameter 수: 1200\n",
      "\n",
      "1차원으로 펼친 데이터: (64, 1327104)\n",
      "\n",
      "Linear 결과: (64, 1)\n",
      "Linear Layer의 Parameter 수: 1327104\n"
     ]
    }
   ],
   "source": [
    "# Convolution 레이어\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "pic = tf.zeros((batch_size, 1920, 1080, 3))\n",
    "\n",
    "print(\"입력 이미지 데이터:\", pic.shape)\n",
    "conv_layer = tf.keras.layers.Conv2D(filters=16,\n",
    "                                    kernel_size=(5, 5),\n",
    "                                    strides=5,\n",
    "                                    use_bias=False)\n",
    "conv_out = conv_layer(pic)\n",
    "\n",
    "print(\"\\nConvolution 결과:\", conv_out.shape)\n",
    "print(\"Convolution Layer의 Parameter 수:\", conv_layer.count_params())\n",
    "\n",
    "flatten_out = tf.keras.layers.Flatten()(conv_out)\n",
    "print(\"\\n1차원으로 펼친 데이터:\", flatten_out.shape)\n",
    "\n",
    "linear_layer = tf.keras.layers.Dense(units=1, use_bias=False)\n",
    "linear_out = linear_layer(flatten_out)\n",
    "\n",
    "print(\"\\nLinear 결과:\", linear_out.shape)\n",
    "print(\"Linear Layer의 Parameter 수:\", linear_layer.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a7991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution 레이어로 손실된 데이터 복원하는 Auto Encoder\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "import json\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "\n",
    "# MNIST 데이터 로딩\n",
    "(x_train, _), (x_test, _) = mnist.load_data()    # y_train, y_test는 사용하지 않습니다.\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1272581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 4)           292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 4)           148       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 8)           296       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 3,369\n",
      "Trainable params: 3,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# AutoEncoder 모델 구성 - Input 부분\n",
    "input_shape = x_train.shape[1:]\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Encoder 부분\n",
    "encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_1 = MaxPooling2D((2, 2), padding='same')\n",
    "encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_2 = MaxPooling2D((2, 2), padding='same')\n",
    "encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu', padding='same')\n",
    "encode_pool_layer_3 = MaxPooling2D((2, 2), padding='same')\n",
    "\n",
    "encoded = encode_conv_layer_1(input_img)\n",
    "encoded = encode_pool_layer_1(encoded)\n",
    "encoded = encode_conv_layer_2(encoded)\n",
    "encoded = encode_pool_layer_2(encoded)\n",
    "encoded = encode_conv_layer_3(encoded)\n",
    "encoded = encode_pool_layer_3(encoded)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Decoder 부분\n",
    "decode_conv_layer_1 = Conv2D(4, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_1 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_2 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_3 = Conv2D(16, (3, 3), activation='relu')\n",
    "decode_upsample_layer_3 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_4 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "decoded = decode_conv_layer_1(encoded)   # Decoder는 Encoder의 출력을 입력으로 받습니다.\n",
    "decoded = decode_upsample_layer_1(decoded)\n",
    "decoded = decode_conv_layer_2(decoded)\n",
    "decoded = decode_upsample_layer_2(decoded)\n",
    "decoded = decode_conv_layer_3(decoded)\n",
    "decoded = decode_upsample_layer_3(decoded)\n",
    "decoded = decode_conv_layer_4(decoded)\n",
    "\n",
    "# AutoEncoder 모델 정의\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd1172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 00:08:23.585405: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-04 00:08:23.585586: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation model/conv2d_1/Conv2D/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node model/conv2d_1/Conv2D/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nResourceApplyAdadelta: CPU \nReadVariableOp: GPU CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  model_conv2d_1_conv2d_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adadelta_adadelta_update_resourceapplyadadelta_accum (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adadelta_adadelta_update_resourceapplyadadelta_accum_update (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  model/conv2d_1/Conv2D/ReadVariableOp (ReadVariableOp) \n  Adadelta/Adadelta/update/ResourceApplyAdadelta (ResourceApplyAdadelta) /job:localhost/replica:0/task:0/device:GPU:0\n\n\t [[{{node model/conv2d_1/Conv2D/ReadVariableOp}}]] [Op:__inference_train_function_1145]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xb/k5k3tvhd5rnc7_tf5mv_3xs00000gn/T/ipykernel_23312/3017280046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adadelta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m autoencoder.fit(x_train, x_train,\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/hs/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation model/conv2d_1/Conv2D/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node model/conv2d_1/Conv2D/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nResourceApplyAdadelta: CPU \nReadVariableOp: GPU CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  model_conv2d_1_conv2d_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adadelta_adadelta_update_resourceapplyadadelta_accum (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  adadelta_adadelta_update_resourceapplyadadelta_accum_update (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  model/conv2d_1/Conv2D/ReadVariableOp (ReadVariableOp) \n  Adadelta/Adadelta/update/ResourceApplyAdadelta (ResourceApplyAdadelta) /job:localhost/replica:0/task:0/device:GPU:0\n\n\t [[{{node model/conv2d_1/Conv2D/ReadVariableOp}}]] [Op:__inference_train_function_1145]"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec866c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_10 = x_test[:10]       # 테스트 데이터셋에서 10개만 골라서\n",
    "x_test_hat = autoencoder.predict(x_test_10)    # AutoEncoder 모델의 이미지 복원생성\n",
    "x_test_imgs = x_test_10.reshape(-1, 28, 28)\n",
    "x_test_hat_imgs = x_test_hat.reshape(-1, 28, 28)\n",
    "\n",
    "plt.figure(figsize=(12,5))  # 이미지 사이즈 지정\n",
    "for i in range(10):  \n",
    "    # 원본이미지 출력\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(x_test_imgs[i])\n",
    "    # 생성된 이미지 출력\n",
    "    plt.subplot(2, 10, i+11)\n",
    "    plt.imshow(x_test_hat_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposed Convolution 활용\n",
    "from tensorflow.python.keras.layers import Conv2DTranspose\n",
    "\n",
    "# Conv2DTranspose를 활용한  AutoEncoder 모델\n",
    "# AutoEncoder 모델 구성 - Input 부분\n",
    "input_shape = x_train.shape[1:]\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Encoder 부분\n",
    "encode_conv_layer_1 = Conv2D(16, (3, 3), activation='relu')\n",
    "encode_pool_layer_1 = MaxPooling2D((2, 2))\n",
    "encode_conv_layer_2 = Conv2D(8, (3, 3), activation='relu')\n",
    "encode_pool_layer_2 = MaxPooling2D((2, 2))\n",
    "encode_conv_layer_3 = Conv2D(4, (3, 3), activation='relu')\n",
    "\n",
    "encoded = encode_conv_layer_1(input_img)\n",
    "encoded = encode_pool_layer_1(encoded)\n",
    "encoded = encode_conv_layer_2(encoded)\n",
    "encoded = encode_pool_layer_2(encoded)\n",
    "encoded = encode_conv_layer_3(encoded)\n",
    "\n",
    "# AutoEncoder 모델 구성 - Decoder 부분  - \n",
    "decode_conv_layer_1 = Conv2DTranspose(4, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_1 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_2 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')\n",
    "decode_upsample_layer_2 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_3 = Conv2DTranspose(16, (3, 3), activation='relu')\n",
    "decode_upsample_layer_3 = UpSampling2D((2, 2))\n",
    "decode_conv_layer_4 = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "decoded = decode_conv_layer_1(encoded)   # Decoder는 Encoder의 출력을 입력으로 받습니다.\n",
    "decoded = decode_upsample_layer_1(decoded)\n",
    "decoded = decode_conv_layer_2(decoded)\n",
    "decoded = decode_upsample_layer_2(decoded)\n",
    "decoded = decode_conv_layer_3(decoded)\n",
    "decoded = decode_upsample_layer_3(decoded)\n",
    "decoded = decode_conv_layer_4(decoded)\n",
    "\n",
    "# AutoEncoder 모델 정의\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac260a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721eb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5afb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c45c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hs",
   "language": "python",
   "name": "hs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
